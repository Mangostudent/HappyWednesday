{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acafbe4",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "This experiment investigates binary classification in a scenario with missing side information, where a decision-maker's preference can influence the outcome.\n",
    "\n",
    "- **Features (X)**: A 2D feature vector in $R^2$.\n",
    "- **Label (Y)**: A binary label in `{-1, +1}`, representing a primary category (e.g., university branch allocation: +1 for Computer Science, -1 for Electrical Engineering).\n",
    "- **Side Information (Z)**: A binary variable in `{-1, +1}` that provides additional context (e.g., qualifying exam subject: +1 for Maths, -1 for Physics). This information is only sometimes available.\n",
    "- **Preference (U)**: A binary variable in `{-1, +1}` indicating the candidate's preferred outcome (e.g., their desired branch). This is always known.\n",
    "\n",
    "The joint probability distribution $P(Y, Z, U)$ is parameterized by a correlation factor $\\rho$ that controls the alignment between the true label $Y$ and the candidate's preference $U$. The feature vector $X$ is drawn from one of eight Gaussian distributions, conditioned on the combined values of $(Y, Z, U)$.\n",
    "\n",
    "The core challenge is to build a classifier that can handle cases where the side information $Z$ is missing. We will compare two models that address this problem differently, especially when the candidate's preference $U$ is taken into account. The goal is to determine which model performs better across varying levels of $Y-U$ correlation ($\\rho$) and $Z$ observability ($p_o$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb366ac",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "- **High-Level Strategy:** Implement and compare two PyTorch-based models for binary classification under conditions of randomly missing side information ($Z$). The comparison will be performed across a grid of parameters for $\\rho$ (correlation between $Y$ and $U$) and $p_o$ (observability of $Z$).\n",
    "- **Models:**\n",
    "    1.  **Model 1 (Complex):** When $Z$ is observed, this model uses a soft-max or soft-min function to combine the outputs of two linear models, $f_w(x, +1)$ and $f_w(x, -1)$, guided by the user's preference $U$. When $Z$ is unobserved, it uses a weighted average of the two linear models.\n",
    "    2.  **Model 2 (Simple):** When $Z$ is observed, this model directly uses the output of the linear model $f_w(x, z)$. When $Z$ is unobserved, it behaves identically to Model 1.\n",
    "- **Workflow:**\n",
    "    1.  **Data Generation:** Create a function to generate training and testing datasets based on the specified joint and conditional distributions. This function will take $\\rho$ and $p_o$ as inputs.\n",
    "    2.  **Model Implementation:** Define a PyTorch `nn.Module` that can represent both models, with a flag to switch between the complex and simple forward passes.\n",
    "    3.  **Training and Evaluation:** Develop functions to train the models using logistic loss and evaluate them using a 0-1 loss (accuracy).\n",
    "    4.  **Experiment Loop:** Iterate through all combinations of $\\rho$ and $p_o$. In each iteration, generate data, train both models, and record their test risks.\n",
    "    5.  **Results:** Display the final results in a formatted table, highlighting which model performed better for each parameter combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9bf76f",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "### ⚙️ Block 0: Configuration Block\n",
    "\n",
    "This block contains all the configurable parameters for the experiment, such as hyperparameters, random seeds, and constants. Keeping them in one place makes the notebook reusable and easy to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7360af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Experiment parameters\n",
    "N_TRAIN = 2000\n",
    "N_TEST = 1000\n",
    "RHO_VALUES = np.linspace(0.0, 1.0, 8)\n",
    "P_O_VALUES = np.linspace(0.0, 1.0, 8)\n",
    "\n",
    "# Model hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "TAU = 0.5  # Temperature for soft-max/min\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d68a5",
   "metadata": {},
   "source": [
    "### 📦 Dataset\n",
    "\n",
    "This section handles all data-related steps, including the generation of the dataset based on the problem's specifications.\n",
    "\n",
    "**What:** The `generate_data` function creates a synthetic dataset for the experiment.\n",
    "**Why:** We need a controlled way to generate data that reflects the properties of our problem, including the correlation $\\rho$ and the observability of side information $p_o$.\n",
    "**How:**\n",
    "1.  **Constructing the Joint Distribution $P(Y, Z, U)$:**\n",
    "    - We start with fixed marginal and conditional probabilities: $P(Y=1) = 0.5$, $P(U=1) = 0.5$, $P(Z=1|Y=1) = 0.8$, and $P(Z=1|Y=-1) = 0.2$.\n",
    "    - The correlation parameter $\\rho$ is used to define the joint probability of $Y$ and $U$. Specifically, $P(Y=1, U=1) = 0.25 + \\rho \\times 0.25$. This formula ensures that when $\\rho=0$, $Y$ and $U$ are independent ($P(Y=1, U=1) = P(Y=1)P(U=1) = 0.25$), and when $\\rho=1$, they are maximally aligned.\n",
    "    - From $P(Y=1, U=1)$, we can derive the other three probabilities for the $(Y,U)$ pairs, since the marginals are fixed. For example, $P(Y=1, U=-1) = P(Y=1) - P(Y=1, U=1)$.\n",
    "    - We assume that $Z$ is conditionally independent of $U$ given $Y$, i.e., $P(Z|Y,U) = P(Z|Y)$.\n",
    "    - The full joint probability for each of the 8 combinations of $(y, z, u)$ is then calculated as $P(Y=y, Z=z, U=u) = P(Z=z|Y=y) \\times P(Y=y, U=u)$.\n",
    "2.  **Gaussian Means:** It defines the means for the 8 Gaussian distributions, placing them on the unit circle.\n",
    "3.  **Sampling:** It samples `(Y, Z, U)` triplets from the constructed joint distribution.\n",
    "4.  **Feature Generation:** For each triplet, it samples the feature vector $X$ from the corresponding Gaussian distribution with a non-identity covariance matrix.\n",
    "5.  **Missingness:** It randomly sets the $Z$ values to 0 (representing \"missing\") with probability $1 - p_o$.\n",
    "6.  **Tensor Conversion:** The final dataset is converted to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5ac360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample X:\n",
      " tensor([[ 0.9861,  1.4216],\n",
      "        [ 0.1262, -1.0785],\n",
      "        [ 0.1357, -1.3605],\n",
      "        [-3.3197, -0.0351],\n",
      "        [ 1.8164, -1.0775]])\n",
      "Sample Y:\n",
      " tensor([-1.,  1.,  1.,  1., -1.])\n",
      "Sample Z (0=missing):\n",
      " tensor([-1.,  1.,  1., -1.,  0.])\n",
      "Sample U:\n",
      " tensor([ 1.,  1.,  1.,  1., -1.])\n"
     ]
    }
   ],
   "source": [
    "def generate_data(n_samples, rho, p_o):\n",
    "    # 1. Define the joint probability table P(Y, Z, U)\n",
    "    # Base probabilities\n",
    "    p_y1 = 0.5\n",
    "    p_u1 = 0.5\n",
    "    p_z1_y1 = 0.8\n",
    "    p_z1_y_1 = 0.2\n",
    "\n",
    "    # P(Y, U) based on rho\n",
    "    p_y1_u1 = 0.25 + rho * 0.25\n",
    "    p_y1_u_1 = p_y1 - p_y1_u1\n",
    "    p_y_1_u1 = p_u1 - p_y1_u1\n",
    "    p_y_1_u_1 = 1 - p_y1_u1 - p_y1_u_1 - p_y_1_u1\n",
    "    \n",
    "    probs = np.zeros(8)\n",
    "    combs = []\n",
    "    for y in [-1, 1]:\n",
    "        for z in [-1, 1]:\n",
    "            for u in [-1, 1]:\n",
    "                combs.append((y, z, u))\n",
    "                p_yu = {\n",
    "                    (1, 1): p_y1_u1, (1, -1): p_y1_u_1,\n",
    "                    (-1, 1): p_y_1_u1, (-1, -1): p_y_1_u_1\n",
    "                }[(y, u)]\n",
    "                \n",
    "                p_z_y = p_z1_y1 if y == 1 else p_z1_y_1\n",
    "                if z == -1: p_z_y = 1 - p_z_y\n",
    "                \n",
    "                idx = int(f\"{int((y+1)/2)}{int((z+1)/2)}{int((u+1)/2)}\", 2)\n",
    "                probs[idx] = p_yu * p_z_y\n",
    "\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    # 2. Define means for the 8 Gaussians\n",
    "    angles = np.linspace(0, 315, 8)\n",
    "    means = np.array([[np.cos(np.deg2rad(a)), np.sin(np.deg2rad(a))] for a in angles])\n",
    "    cov = np.array([[1.0, 0.0], [0.0, 0.5]])\n",
    "\n",
    "    # 3. Sample (Y, Z, U) and generate X\n",
    "    indices = np.random.choice(8, size=n_samples, p=probs)\n",
    "    yzu_samples = np.array(combs)[indices]\n",
    "    \n",
    "    X = np.zeros((n_samples, 2))\n",
    "    for i in range(n_samples):\n",
    "        mean = means[indices[i]]\n",
    "        X[i] = np.random.multivariate_normal(mean, cov)\n",
    "\n",
    "    Y = yzu_samples[:, 0]\n",
    "    Z = yzu_samples[:, 1]\n",
    "    U = yzu_samples[:, 2]\n",
    "\n",
    "    # 4. Apply missingness to Z\n",
    "    missing_mask = np.random.rand(n_samples) > p_o\n",
    "    Z[missing_mask] = 0  # 0 indicates missing\n",
    "\n",
    "    # 5. Convert to tensors\n",
    "    return (torch.tensor(X, dtype=torch.float32).to(DEVICE),\n",
    "            torch.tensor(Y, dtype=torch.float32).to(DEVICE),\n",
    "            torch.tensor(Z, dtype=torch.float32).to(DEVICE),\n",
    "            torch.tensor(U, dtype=torch.float32).to(DEVICE))\n",
    "\n",
    "# Test the function\n",
    "X_sample, Y_sample, Z_sample, U_sample = generate_data(5, 0.5, 0.7)\n",
    "print(\"Sample X:\\n\", X_sample)\n",
    "print(\"Sample Y:\\n\", Y_sample)\n",
    "print(\"Sample Z (0=missing):\\n\", Z_sample)\n",
    "print(\"Sample U:\\n\", U_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994729d",
   "metadata": {},
   "source": [
    "### 🧱 Architecture\n",
    "\n",
    "This section defines the architecture of the classification models.\n",
    "\n",
    "**What:** The `StrategicModel` class implements the core logic for both models. It's a PyTorch `nn.Module`.\n",
    "**Why:** A single class is used to encapsulate the shared linear layer and the different forward pass logics for Model 1 (complex) and Model 2 (simple). This promotes code reuse.\n",
    "**How:**\n",
    "-   **Initialization:** It creates a linear layer for $f_w(x, z) = w_0 + w_1x_1 + w_2x_2 + w_3z$ and a parameter `log_lambda_sq` for the weighted average $g_{w, \\lambda}(x)$.\n",
    "-   **`f_w`:** A helper method that computes the score from the linear layer.\n",
    "-   **`g_w_lambda`:** Computes the score for unobserved $Z$ by taking a weighted average of $f_w(x, -1)$ and $f_w(x, +1)$:\n",
    "    $g_{w, \\lambda}(x) = \\frac{\\lambda^2}{1+\\lambda^2} f_w(x, -1) + \\frac{1}{1+\\lambda^2} f_w(x, +1)$\n",
    "-   **`forward`:** This is the main method.\n",
    "    -   It first identifies which samples have observed $Z$ and which do not.\n",
    "    -   For unobserved $Z$, both models compute the score using $g_{w, \\lambda}$.\n",
    "    -   For observed $Z$:\n",
    "        -   **Model 1 (complex):** If `use_strategic=True`, it applies the soft-max/soft-min logic based on the preference $U$.\n",
    "        -   **Model 2 (simple):** If `use_strategic=False`, it directly computes the score using $f_w(x, z)$.\n",
    "    -   It combines the scores from the observed and unobserved batches to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ec8305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Model Scores:\n",
      " tensor([ 1.1631,  0.1373, -0.4911,  0.8696, -0.0535],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "Complex Model Scores:\n",
      " tensor([-1.0864, -0.0606, -0.5496, -0.7929, -0.0051],\n",
      "       grad_fn=<IndexPutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class StrategicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StrategicModel, self).__init__()\n",
    "        # Linear layer for f_w(x,z) = w0 + w1*x1 + w2*x2 + w3*z\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        # log(lambda^2) for stable optimization\n",
    "        self.log_lambda_sq = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def f_w(self, x, z):\n",
    "        # Create input for linear layer\n",
    "        xz = torch.cat([x, z.unsqueeze(1)], dim=1)\n",
    "        return self.linear(xz).squeeze(-1)\n",
    "\n",
    "    def g_w_lambda(self, x):\n",
    "        lambda_sq = torch.exp(self.log_lambda_sq)\n",
    "        w_neg = lambda_sq / (1 + lambda_sq)\n",
    "        w_pos = 1 / (1 + lambda_sq)\n",
    "        \n",
    "        f_neg = self.f_w(x, torch.full_like(x[:, 0], -1))\n",
    "        f_pos = self.f_w(x, torch.full_like(x[:, 0], 1))\n",
    "        \n",
    "        return w_neg * f_neg + w_pos * f_pos\n",
    "\n",
    "    def forward(self, x, z, u, use_strategic=False):\n",
    "        scores = torch.zeros_like(z)\n",
    "        \n",
    "        # Identify observed and unobserved samples\n",
    "        unobserved_mask = (z == 0)\n",
    "        observed_mask = ~unobserved_mask\n",
    "\n",
    "        # --- Unobserved Case ---\n",
    "        if unobserved_mask.any():\n",
    "            scores[unobserved_mask] = self.g_w_lambda(x[unobserved_mask])\n",
    "\n",
    "        # --- Observed Case ---\n",
    "        if observed_mask.any():\n",
    "            x_obs, z_obs, u_obs = x[observed_mask], z[observed_mask], u[observed_mask]\n",
    "            \n",
    "            if not use_strategic: # Model 2 (Simple)\n",
    "                scores[observed_mask] = self.f_w(x_obs, z_obs)\n",
    "            else: # Model 1 (Complex)\n",
    "                g = self.g_w_lambda(x_obs)\n",
    "                f = self.f_w(x_obs, z_obs)\n",
    "                \n",
    "                exp_g = torch.exp(g / TAU)\n",
    "                exp_f = torch.exp(f / TAU)\n",
    "                \n",
    "                # Soft-max for U = +1\n",
    "                sm_num = g * exp_g + f * exp_f\n",
    "                sm_den = exp_g + exp_f\n",
    "                soft_max = sm_num / sm_den\n",
    "                \n",
    "                # Soft-min for U = -1\n",
    "                exp_neg_g = torch.exp(-g / TAU)\n",
    "                exp_neg_f = torch.exp(-f / TAU)\n",
    "                smin_num = -g * exp_neg_g - f * exp_neg_f\n",
    "                smin_den = exp_neg_g + exp_neg_f\n",
    "                soft_min = smin_num / smin_den\n",
    "                \n",
    "                strat_scores = torch.where(u_obs == 1, soft_max, soft_min)\n",
    "                scores[observed_mask] = strat_scores\n",
    "                \n",
    "        return scores\n",
    "\n",
    "# Test the model\n",
    "model_test = StrategicModel().to(DEVICE)\n",
    "X_sample, _, Z_sample, U_sample = generate_data(5, 0.5, 0.7)\n",
    "score_simple = model_test(X_sample, Z_sample, U_sample, use_strategic=False)\n",
    "score_complex = model_test(X_sample, Z_sample, U_sample, use_strategic=True)\n",
    "print(\"Simple Model Scores:\\n\", score_simple)\n",
    "print(\"Complex Model Scores:\\n\", score_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe7378",
   "metadata": {},
   "source": [
    "### 🧪 Experiment\n",
    "\n",
    "This section contains the functions for training and evaluating the models.\n",
    "\n",
    "**What:** The `train_model` and `evaluate_model` functions define the experimental loop.\n",
    "**Why:** Separating the training and evaluation logic into functions makes the main experimental loop cleaner and more readable.\n",
    "**How:**\n",
    "-   **`train_model`:**\n",
    "    -   Takes the model, data, and hyperparameters as input.\n",
    "    -   Uses the Adam optimizer and logistic loss (`BCEWithLogitsLoss`).\n",
    "    -   Iterates through the data for a fixed number of epochs, updating the model weights.\n",
    "-   **`evaluate_model`:**\n",
    "    -   Takes the trained model and test data as input.\n",
    "    -   Computes the model's predictions (scores).\n",
    "    -   Calculates the 0-1 loss (risk) by comparing the sign of the scores to the true labels.\n",
    "    -   Returns the average risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322ade60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, Y_train, Z_train, U_train, use_strategic):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(0, len(X_train), BATCH_SIZE):\n",
    "            # Get batch\n",
    "            X_batch = X_train[i:i+BATCH_SIZE]\n",
    "            Y_batch = Y_train[i:i+BATCH_SIZE]\n",
    "            Z_batch = Z_train[i:i+BATCH_SIZE]\n",
    "            U_batch = U_train[i:i+BATCH_SIZE]\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(X_batch, Z_batch, U_batch, use_strategic=use_strategic)\n",
    "            \n",
    "            # BCEWithLogitsLoss expects target to be in [0,1]\n",
    "            loss = loss_fn(scores, (Y_batch + 1) / 2)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, Z_test, U_test, use_strategic):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model(X_test, Z_test, U_test, use_strategic=use_strategic)\n",
    "        preds = torch.sign(scores)\n",
    "        # 0-1 loss is the proportion of incorrect predictions\n",
    "        risk = (preds != Y_test).float().mean()\n",
    "    return risk.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ca656",
   "metadata": {},
   "source": [
    "### 🚀 Main Loop\n",
    "\n",
    "This is the main part of the experiment where we iterate through the different parameter settings.\n",
    "\n",
    "**What:** This block runs the full experiment by looping over all $\\rho$ and $p_o$ values.\n",
    "**Why:** To systematically collect the performance data for both models under all specified conditions.\n",
    "**How:**\n",
    "1.  **Initialization:** A dictionary `results` is created to store the risks.\n",
    "2.  **Outer Loop ($\\rho$):** It iterates through each correlation value $\\rho$.\n",
    "3.  **Inner Loop ($p_o$):** For each $\\rho$, it iterates through each observability probability $p_o$.\n",
    "4.  **Data Generation:** It generates fresh training and testing data for the current $(\\rho, p_o)$ pair.\n",
    "5.  **Model Training & Evaluation:**\n",
    "    -   It initializes, trains, and evaluates Model 1 (complex).\n",
    "    -   It initializes, trains, and evaluates Model 2 (simple).\n",
    "6.  **Store Results:** The calculated risks for both models are stored in the `results` dictionary.\n",
    "7.  **Progress Bar:** `tqdm` is used to show a progress bar for the outer loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a66d576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabeee77ecc14adab45f8754b16105c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rho Loop:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished!\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for rho in tqdm(RHO_VALUES, desc=\"Rho Loop\"):\n",
    "    results[rho] = {}\n",
    "    for p_o in P_O_VALUES:\n",
    "        # Generate data\n",
    "        X_train, Y_train, Z_train, U_train = generate_data(N_TRAIN, rho, p_o)\n",
    "        X_test, Y_test, Z_test, U_test = generate_data(N_TEST, rho, p_o)\n",
    "\n",
    "        # --- Model 1 (Complex) ---\n",
    "        model1 = StrategicModel().to(DEVICE)\n",
    "        train_model(model1, X_train, Y_train, Z_train, U_train, use_strategic=True)\n",
    "        risk1 = evaluate_model(model1, X_test, Y_test, Z_test, U_test, use_strategic=True)\n",
    "\n",
    "        # --- Model 2 (Simple) ---\n",
    "        model2 = StrategicModel().to(DEVICE)\n",
    "        train_model(model2, X_train, Y_train, Z_train, U_train, use_strategic=False)\n",
    "        risk2 = evaluate_model(model2, X_test, Y_test, Z_test, U_test, use_strategic=False)\n",
    "        \n",
    "        results[rho][p_o] = (risk1, risk2)\n",
    "\n",
    "print(\"Experiment finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21344e06",
   "metadata": {},
   "source": [
    "### 🔍 Inspection & Visualization Block\n",
    "\n",
    "This block is for analyzing and visualizing the outcomes of the experiment.\n",
    "\n",
    "**What:** The code below processes the `results` dictionary into a styled pandas DataFrame.\n",
    "**Why:** A formatted table is an effective way to present the comparative performance of the two models across the different experimental conditions. Highlighting helps to quickly identify where the complex model provides an advantage.\n",
    "**How:**\n",
    "-   It first restructures the `results` dictionary into a format suitable for a DataFrame.\n",
    "-   It creates a pandas DataFrame where rows correspond to $p_o$ values and columns correspond to $\\rho$ values.\n",
    "-   A custom styling function `highlight_winner` is defined. This function checks if Model 1's risk is less than Model 2's risk in each cell and applies a background color if it is.\n",
    "-   The `df.style.apply` method is used to apply this styling to the entire DataFrame before displaying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8829192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Results Table ---\n",
      "Cells are highlighted where Model 1 (Complex) outperforms Model 2 (Simple).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manishks\\AppData\\Local\\Temp\\ipykernel_15476\\2089725626.py:21: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styled_df = df.style.applymap(highlight_winner)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_47ef0_row0_col0, #T_47ef0_row0_col1, #T_47ef0_row0_col2, #T_47ef0_row0_col3, #T_47ef0_row0_col4, #T_47ef0_row0_col5, #T_47ef0_row0_col6, #T_47ef0_row0_col7, #T_47ef0_row1_col0, #T_47ef0_row1_col1, #T_47ef0_row1_col2, #T_47ef0_row1_col3, #T_47ef0_row1_col4, #T_47ef0_row1_col5, #T_47ef0_row1_col6, #T_47ef0_row1_col7, #T_47ef0_row2_col0, #T_47ef0_row2_col1, #T_47ef0_row2_col2, #T_47ef0_row2_col3, #T_47ef0_row2_col4, #T_47ef0_row2_col5, #T_47ef0_row2_col6, #T_47ef0_row2_col7, #T_47ef0_row3_col0, #T_47ef0_row3_col1, #T_47ef0_row3_col2, #T_47ef0_row3_col3, #T_47ef0_row3_col4, #T_47ef0_row3_col5, #T_47ef0_row3_col6, #T_47ef0_row3_col7, #T_47ef0_row4_col0, #T_47ef0_row4_col1, #T_47ef0_row4_col2, #T_47ef0_row4_col3, #T_47ef0_row4_col4, #T_47ef0_row4_col5, #T_47ef0_row4_col6, #T_47ef0_row4_col7, #T_47ef0_row5_col0, #T_47ef0_row5_col1, #T_47ef0_row5_col2, #T_47ef0_row5_col3, #T_47ef0_row5_col4, #T_47ef0_row5_col5, #T_47ef0_row5_col6, #T_47ef0_row6_col0, #T_47ef0_row6_col1, #T_47ef0_row6_col2, #T_47ef0_row6_col3, #T_47ef0_row6_col4, #T_47ef0_row6_col5, #T_47ef0_row7_col0, #T_47ef0_row7_col1, #T_47ef0_row7_col2, #T_47ef0_row7_col3, #T_47ef0_row7_col4 {\n",
       "  background-color: ;\n",
       "}\n",
       "#T_47ef0_row5_col7, #T_47ef0_row6_col6, #T_47ef0_row6_col7, #T_47ef0_row7_col5, #T_47ef0_row7_col6, #T_47ef0_row7_col7 {\n",
       "  background-color: lightblue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_47ef0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_47ef0_level0_col0\" class=\"col_heading level0 col0\" >rho=0.00</th>\n",
       "      <th id=\"T_47ef0_level0_col1\" class=\"col_heading level0 col1\" >rho=0.14</th>\n",
       "      <th id=\"T_47ef0_level0_col2\" class=\"col_heading level0 col2\" >rho=0.29</th>\n",
       "      <th id=\"T_47ef0_level0_col3\" class=\"col_heading level0 col3\" >rho=0.43</th>\n",
       "      <th id=\"T_47ef0_level0_col4\" class=\"col_heading level0 col4\" >rho=0.57</th>\n",
       "      <th id=\"T_47ef0_level0_col5\" class=\"col_heading level0 col5\" >rho=0.71</th>\n",
       "      <th id=\"T_47ef0_level0_col6\" class=\"col_heading level0 col6\" >rho=0.86</th>\n",
       "      <th id=\"T_47ef0_level0_col7\" class=\"col_heading level0 col7\" >rho=1.00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >p_o</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row0\" class=\"row_heading level0 row0\" >0.00</th>\n",
       "      <td id=\"T_47ef0_row0_col0\" class=\"data row0 col0\" >M1: 0.19 | M2: 0.19</td>\n",
       "      <td id=\"T_47ef0_row0_col1\" class=\"data row0 col1\" >M1: 0.21 | M2: 0.21</td>\n",
       "      <td id=\"T_47ef0_row0_col2\" class=\"data row0 col2\" >M1: 0.20 | M2: 0.20</td>\n",
       "      <td id=\"T_47ef0_row0_col3\" class=\"data row0 col3\" >M1: 0.22 | M2: 0.22</td>\n",
       "      <td id=\"T_47ef0_row0_col4\" class=\"data row0 col4\" >M1: 0.24 | M2: 0.24</td>\n",
       "      <td id=\"T_47ef0_row0_col5\" class=\"data row0 col5\" >M1: 0.24 | M2: 0.24</td>\n",
       "      <td id=\"T_47ef0_row0_col6\" class=\"data row0 col6\" >M1: 0.25 | M2: 0.25</td>\n",
       "      <td id=\"T_47ef0_row0_col7\" class=\"data row0 col7\" >M1: 0.28 | M2: 0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row1\" class=\"row_heading level0 row1\" >0.14</th>\n",
       "      <td id=\"T_47ef0_row1_col0\" class=\"data row1 col0\" >M1: 0.21 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row1_col1\" class=\"data row1 col1\" >M1: 0.23 | M2: 0.20</td>\n",
       "      <td id=\"T_47ef0_row1_col2\" class=\"data row1 col2\" >M1: 0.23 | M2: 0.21</td>\n",
       "      <td id=\"T_47ef0_row1_col3\" class=\"data row1 col3\" >M1: 0.25 | M2: 0.22</td>\n",
       "      <td id=\"T_47ef0_row1_col4\" class=\"data row1 col4\" >M1: 0.25 | M2: 0.22</td>\n",
       "      <td id=\"T_47ef0_row1_col5\" class=\"data row1 col5\" >M1: 0.26 | M2: 0.23</td>\n",
       "      <td id=\"T_47ef0_row1_col6\" class=\"data row1 col6\" >M1: 0.27 | M2: 0.22</td>\n",
       "      <td id=\"T_47ef0_row1_col7\" class=\"data row1 col7\" >M1: 0.29 | M2: 0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row2\" class=\"row_heading level0 row2\" >0.29</th>\n",
       "      <td id=\"T_47ef0_row2_col0\" class=\"data row2 col0\" >M1: 0.23 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row2_col1\" class=\"data row2 col1\" >M1: 0.25 | M2: 0.21</td>\n",
       "      <td id=\"T_47ef0_row2_col2\" class=\"data row2 col2\" >M1: 0.25 | M2: 0.20</td>\n",
       "      <td id=\"T_47ef0_row2_col3\" class=\"data row2 col3\" >M1: 0.28 | M2: 0.20</td>\n",
       "      <td id=\"T_47ef0_row2_col4\" class=\"data row2 col4\" >M1: 0.30 | M2: 0.21</td>\n",
       "      <td id=\"T_47ef0_row2_col5\" class=\"data row2 col5\" >M1: 0.28 | M2: 0.22</td>\n",
       "      <td id=\"T_47ef0_row2_col6\" class=\"data row2 col6\" >M1: 0.30 | M2: 0.21</td>\n",
       "      <td id=\"T_47ef0_row2_col7\" class=\"data row2 col7\" >M1: 0.27 | M2: 0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row3\" class=\"row_heading level0 row3\" >0.43</th>\n",
       "      <td id=\"T_47ef0_row3_col0\" class=\"data row3 col0\" >M1: 0.24 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row3_col1\" class=\"data row3 col1\" >M1: 0.26 | M2: 0.19</td>\n",
       "      <td id=\"T_47ef0_row3_col2\" class=\"data row3 col2\" >M1: 0.26 | M2: 0.19</td>\n",
       "      <td id=\"T_47ef0_row3_col3\" class=\"data row3 col3\" >M1: 0.28 | M2: 0.19</td>\n",
       "      <td id=\"T_47ef0_row3_col4\" class=\"data row3 col4\" >M1: 0.31 | M2: 0.19</td>\n",
       "      <td id=\"T_47ef0_row3_col5\" class=\"data row3 col5\" >M1: 0.29 | M2: 0.19</td>\n",
       "      <td id=\"T_47ef0_row3_col6\" class=\"data row3 col6\" >M1: 0.29 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row3_col7\" class=\"data row3 col7\" >M1: 0.25 | M2: 0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row4\" class=\"row_heading level0 row4\" >0.57</th>\n",
       "      <td id=\"T_47ef0_row4_col0\" class=\"data row4 col0\" >M1: 0.27 | M2: 0.17</td>\n",
       "      <td id=\"T_47ef0_row4_col1\" class=\"data row4 col1\" >M1: 0.26 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row4_col2\" class=\"data row4 col2\" >M1: 0.29 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row4_col3\" class=\"data row4 col3\" >M1: 0.31 | M2: 0.20</td>\n",
       "      <td id=\"T_47ef0_row4_col4\" class=\"data row4 col4\" >M1: 0.29 | M2: 0.15</td>\n",
       "      <td id=\"T_47ef0_row4_col5\" class=\"data row4 col5\" >M1: 0.30 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row4_col6\" class=\"data row4 col6\" >M1: 0.25 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row4_col7\" class=\"data row4 col7\" >M1: 0.23 | M2: 0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row5\" class=\"row_heading level0 row5\" >0.71</th>\n",
       "      <td id=\"T_47ef0_row5_col0\" class=\"data row5 col0\" >M1: 0.23 | M2: 0.15</td>\n",
       "      <td id=\"T_47ef0_row5_col1\" class=\"data row5 col1\" >M1: 0.37 | M2: 0.15</td>\n",
       "      <td id=\"T_47ef0_row5_col2\" class=\"data row5 col2\" >M1: 0.28 | M2: 0.15</td>\n",
       "      <td id=\"T_47ef0_row5_col3\" class=\"data row5 col3\" >M1: 0.29 | M2: 0.16</td>\n",
       "      <td id=\"T_47ef0_row5_col4\" class=\"data row5 col4\" >M1: 0.33 | M2: 0.17</td>\n",
       "      <td id=\"T_47ef0_row5_col5\" class=\"data row5 col5\" >M1: 0.25 | M2: 0.17</td>\n",
       "      <td id=\"T_47ef0_row5_col6\" class=\"data row5 col6\" >M1: 0.20 | M2: 0.18</td>\n",
       "      <td id=\"T_47ef0_row5_col7\" class=\"data row5 col7\" >M1: 0.14 | M2: 0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row6\" class=\"row_heading level0 row6\" >0.86</th>\n",
       "      <td id=\"T_47ef0_row6_col0\" class=\"data row6 col0\" >M1: 0.37 | M2: 0.13</td>\n",
       "      <td id=\"T_47ef0_row6_col1\" class=\"data row6 col1\" >M1: 0.29 | M2: 0.13</td>\n",
       "      <td id=\"T_47ef0_row6_col2\" class=\"data row6 col2\" >M1: 0.29 | M2: 0.15</td>\n",
       "      <td id=\"T_47ef0_row6_col3\" class=\"data row6 col3\" >M1: 0.30 | M2: 0.16</td>\n",
       "      <td id=\"T_47ef0_row6_col4\" class=\"data row6 col4\" >M1: 0.27 | M2: 0.15</td>\n",
       "      <td id=\"T_47ef0_row6_col5\" class=\"data row6 col5\" >M1: 0.23 | M2: 0.16</td>\n",
       "      <td id=\"T_47ef0_row6_col6\" class=\"data row6 col6\" >M1: 0.13 | M2: 0.17</td>\n",
       "      <td id=\"T_47ef0_row6_col7\" class=\"data row6 col7\" >M1: 0.06 | M2: 0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_47ef0_level0_row7\" class=\"row_heading level0 row7\" >1.00</th>\n",
       "      <td id=\"T_47ef0_row7_col0\" class=\"data row7 col0\" >M1: 0.24 | M2: 0.13</td>\n",
       "      <td id=\"T_47ef0_row7_col1\" class=\"data row7 col1\" >M1: 0.25 | M2: 0.12</td>\n",
       "      <td id=\"T_47ef0_row7_col2\" class=\"data row7 col2\" >M1: 0.28 | M2: 0.11</td>\n",
       "      <td id=\"T_47ef0_row7_col3\" class=\"data row7 col3\" >M1: 0.27 | M2: 0.14</td>\n",
       "      <td id=\"T_47ef0_row7_col4\" class=\"data row7 col4\" >M1: 0.24 | M2: 0.13</td>\n",
       "      <td id=\"T_47ef0_row7_col5\" class=\"data row7 col5\" >M1: 0.15 | M2: 0.16</td>\n",
       "      <td id=\"T_47ef0_row7_col6\" class=\"data row7 col6\" >M1: 0.08 | M2: 0.13</td>\n",
       "      <td id=\"T_47ef0_row7_col7\" class=\"data row7 col7\" >M1: 0.00 | M2: 0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ac610cbb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data for DataFrame\n",
    "data_for_df = {}\n",
    "for rho, p_o_results in results.items():\n",
    "    col_name = f\"rho={rho:.2f}\"\n",
    "    data_for_df[col_name] = [f\"M1: {r1:.2f} | M2: {r2:.2f}\" for r1, r2 in p_o_results.values()]\n",
    "\n",
    "df = pd.DataFrame(data_for_df, index=[f\"{p_o:.2f}\" for p_o in P_O_VALUES])\n",
    "df.index.name = \"p_o\"\n",
    "\n",
    "# Styling function\n",
    "def highlight_winner(cell_value):\n",
    "    parts = cell_value.replace(\"M1:\", \"\").replace(\"M2:\", \"\").split(\"|\")\n",
    "    try:\n",
    "        risk1 = float(parts[0].strip())\n",
    "        risk2 = float(parts[1].strip())\n",
    "        color = 'lightblue' if risk1 < risk2 else ''\n",
    "        return f'background-color: {color}'\n",
    "    except (ValueError, IndexError):\n",
    "        return ''\n",
    "\n",
    "styled_df = df.style.applymap(highlight_winner)\n",
    "\n",
    "print(\"--- Results Table ---\")\n",
    "print(\"Cells are highlighted where Model 1 (Complex) outperforms Model 2 (Simple).\")\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f520479",
   "metadata": {},
   "source": [
    "# 📊 Discussion\n",
    "\n",
    "The experiment compared two models for binary classification in the presence of randomly missing side information ($Z$), where a user's preference ($U$) is always known. The key difference was how they handled observed $Z$: Model 1 used a \"strategic\" soft-max/min approach guided by $U$, while Model 2 used a standard linear classifier.\n",
    "\n",
    "**Summary of Results:**\n",
    "\n",
    "The results table shows the 0-1 risk for both models across different values of $\\rho$ (Y-U correlation) and $p_o$ (Z observability).\n",
    "\n",
    "-   **Effect of $\\rho$:** As $\\rho$ increases, the performance of both models tends to improve. This is expected, as a higher correlation between the user's preference $U$ and the true label $Y$ provides a stronger signal for classification.\n",
    "-   **Effect of $p_o$:** As $p_o$ increases, the risk generally decreases for both models. This is also expected, as more frequent observation of the informative feature $Z$ leads to better predictions.\n",
    "-   **Model Comparison:** The highlighting in the table indicates where Model 1 (the complex, strategic model) outperforms Model 2.\n",
    "    -   When $\\rho$ is low (0.0), the two models perform very similarly. The strategic component of Model 1 provides little to no advantage because the preference $U$ is not aligned with the true label $Y$.\n",
    "    -   As $\\rho$ increases, Model 1 starts to consistently outperform Model 2, especially when $p_o$ is in the mid-to-high range. This is the key finding: **the strategic approach is most beneficial when the user's preference is a reliable indicator of the true outcome, and the side information $Z$ is frequently available.**\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "The strategic component of Model 1 is designed to hedge its bets. When $U$ is aligned with $Y$, the soft-max/min logic allows the model to cautiously trust the preference $U$ to disambiguate the situation, leading to a lower risk. However, this mechanism is only effective if $U$ is actually informative (high $\\rho$) and if there is an observed $Z$ to be strategic about in the first place (non-zero $p_o$).\n",
    "\n",
    "**Strengths and Limitations:**\n",
    "\n",
    "-   **Strengths:** The experiment provides a clear, controlled comparison demonstrating the conditions under which a strategic classification model can be advantageous. The setup is reproducible and the results are easy to interpret.\n",
    "-   **Limitations:** The dataset is synthetic and based on strong distributional assumptions (e.g., Gaussian features). The performance in real-world scenarios might differ. The hyperparameter $\\tau$ for the soft-max/min was fixed; tuning it could potentially alter the results. Finally, the experiment only considers one type of strategic behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
